"""@package docstring
File IO to load/save tsv/csv using the built-in csv module

This unit is generated by AI chatbot using prompts made by Qianqian Fang
This unit is under the public domain
"""

import csv
import gzip
import zlib
import base64
from typing import Dict, List, Any, Optional, Union
import numpy as np
from collections import OrderedDict

__all__ = [
    "load_csv_tsv",
    "loadcsv",
    "loadtsv",
    "save_csv_tsv",
    "encode_enum_column",
    "decode_enum_column",
    "is_enum_encoded",
    "tsv2json",
    "json2tsv",
    "save_csv_tsv_with_enum",
]

##====================================================================================
## dependent libraries
##====================================================================================


def load_csv_tsv(
    filename: str,
    delimiter: Optional[str] = None,
    return_dict: bool = True,
    convert_numeric: bool = True,
    header: bool = True,
    encoding: str = "utf-8",
    **kwargs,
) -> Union[Dict[str, List], List[List[str]]]:
    """
    Read CSV, TSV, and their gzipped variants using Python's csv module

    Args:
        filename: Path to the file (.csv, .tsv, .csv.gz, .tsv.gz)
        delimiter: Column delimiter. If None, auto-detect from file extension
        return_dict: If True, return dict with column names as keys. If False, return list of rows
        convert_numeric: If True, attempt to convert numeric strings to numbers
        header: If True, treat first row as header
        encoding: File encoding (default: 'utf-8')
        **kwargs: Additional arguments passed to csv.reader/DictReader

    Returns:
        Dict with column names as keys and lists as values, or list of rows

    Examples:
        # Read CSV file as dictionary
        data = read_delimited_file('data.csv')

        # Read TSV file as list of rows
        rows = read_delimited_file('data.tsv', return_dict=False)

        # Read compressed file with custom delimiter
        data = read_delimited_file('data.csv.gz', delimiter=';')
    """

    # Auto-detect delimiter from filename if not provided
    if delimiter is None:
        filename_lower = filename.lower()
        if ".csv" in filename_lower:
            delimiter = ","
        elif ".tsv" in filename_lower:
            delimiter = "\t"
        else:
            delimiter = ","  # Default to comma

    # Determine if file is compressed
    is_compressed = filename.lower().endswith(".gz")

    # Open file (compressed or regular)
    if is_compressed:
        file_handle = gzip.open(filename, "rt", encoding=encoding)
    else:
        file_handle = open(filename, "r", encoding=encoding)

    try:
        # Set up CSV reader parameters
        csv_params = {
            "delimiter": delimiter,
            "quotechar": '"',
            "quoting": csv.QUOTE_MINIMAL,
            **kwargs,
        }

        if return_dict and header:
            # Use DictReader for dictionary output with headers
            reader = csv.DictReader(file_handle, **csv_params)

            # Read all rows
            rows = list(reader)

            if not rows:
                return {}

            # Convert to column-based dictionary
            data = {}
            fieldnames = reader.fieldnames or []

            for field in fieldnames:
                column_data = [row.get(field, "") for row in rows]

                if convert_numeric:
                    column_data = group_column(column_data)

                data[field] = column_data

            return data

        else:
            # Use regular reader for list output
            reader = csv.reader(file_handle, **csv_params)

            # Read all rows
            rows = list(reader)

            if not rows:
                return []

            if convert_numeric:
                # Convert numeric values in each row
                converted_rows = []
                for row in rows:
                    converted_row = [group_column(cell) for cell in row]
                    converted_rows.append(converted_row)
                rows = converted_rows

            if return_dict and not header:
                # Create dictionary with generic column names
                if rows:
                    num_cols = len(rows[0])
                    fieldnames = [f"col_{i}" for i in range(num_cols)]

                    data = {}
                    for i, field in enumerate(fieldnames):
                        column_data = [row[i] if i < len(row) else "" for row in rows]
                        data[field] = column_data

                    return data
                else:
                    return {}

            return rows

    finally:
        file_handle.close()


def tonumbers(value: str) -> Union[int, float, str]:
    """
    Convert a string value to numeric if possible

    Args:
        value: String value to convert

    Returns:
        Converted numeric value or original string
    """

    if not isinstance(value, str):
        return value

    # Handle empty strings
    if not value.strip():
        return value

    # Handle common NA values
    if value.lower() in ["na", "n/a", "nan", "null", "none", ""]:
        return np.nan

    # Try integer conversion first
    try:
        # Check if it looks like an integer (no decimal point)
        if "." not in value and "e" not in value.lower():
            return int(value)
    except (ValueError, TypeError):
        pass

    # Try float conversion
    try:
        return float(value)
    except (ValueError, TypeError):
        pass

    # Return as string if conversion fails
    return value


def group_column(column: List[str]) -> List[Any]:
    """
    Convert a column of string values to numeric where possible

    Args:
        column: List of string values

    Returns:
        List with numeric values converted
    """

    converted = [tonumbers(value) for value in column]

    # Check if all non-NaN values are numeric
    numeric_count = sum(
        1 for x in converted if isinstance(x, (int, float)) and not np.isnan(x)
    )
    nan_count = sum(1 for x in converted if isinstance(x, float) and np.isnan(x))
    total_numeric = numeric_count + nan_count

    # If most values are numeric, convert the whole column to numpy array
    if total_numeric > len(converted) * 0.8:  # 80% threshold
        try:
            # Convert to numpy array, handling mixed types
            numeric_array = []
            for value in converted:
                if isinstance(value, (int, float)):
                    numeric_array.append(value)
                elif isinstance(value, str):
                    try:
                        numeric_array.append(float(value))
                    except (ValueError, TypeError):
                        numeric_array.append(np.nan)
                else:
                    numeric_array.append(np.nan)

            return np.array(numeric_array, dtype=float)
        except Exception:
            pass

    return converted


def loadcsv(filename: str, **kwargs) -> Dict[str, List]:
    """
    Convenience function to read CSV files

    Args:
        filename: CSV file path
        **kwargs: Additional arguments for read_delimited_file

    Returns:
        Dictionary with column data
    """
    return load_csv_tsv(filename, delimiter=",", **kwargs)


def loadtsv(filename: str, **kwargs) -> Dict[str, List]:
    """
    Convenience function to read TSV files

    Args:
        filename: TSV file path
        **kwargs: Additional arguments for read_delimited_file

    Returns:
        Dictionary with column data
    """
    return load_csv_tsv(filename, delimiter="\t", **kwargs)


def save_csv_tsv(
    data: Union[Dict[str, List], List[List]],
    filename: str,
    delimiter: Optional[str] = None,
    compress: bool = False,
    encoding: str = "utf-8",
    **kwargs,
):
    """
    Write data to CSV/TSV files (with optional compression)

    Args:
        data: Dictionary with column data or list of rows
        filename: Output file path
        delimiter: Column delimiter. If None, auto-detect from filename
        compress: If True, compress output with gzip
        encoding: File encoding
        **kwargs: Additional arguments for csv.writer/DictWriter
    """

    # Auto-detect delimiter
    if delimiter is None:
        if ".tsv" in filename.lower():
            delimiter = "\t"
        else:
            delimiter = ","

    # Add .gz extension if compressing
    if compress and not filename.lower().endswith(".gz"):
        filename += ".gz"

    # Open file
    if compress or filename.lower().endswith(".gz"):
        file_handle = gzip.open(filename, "wt", encoding=encoding)
    else:
        file_handle = open(filename, "w", encoding=encoding, newline="")

    try:
        csv_params = {
            "delimiter": delimiter,
            "quotechar": '"',
            "quoting": csv.QUOTE_MINIMAL,
            **kwargs,
        }

        if isinstance(data, dict):
            # Write dictionary data
            fieldnames = list(data.keys())
            writer = csv.DictWriter(file_handle, fieldnames=fieldnames, **csv_params)

            # Write header
            writer.writeheader()

            # Write rows
            if fieldnames:
                num_rows = len(data[fieldnames[0]])
                for i in range(num_rows):
                    row = {
                        field: data[field][i] if i < len(data[field]) else ""
                        for field in fieldnames
                    }
                    writer.writerow(row)

        else:
            # Write list data
            writer = csv.writer(file_handle, **csv_params)
            writer.writerows(data)

    finally:
        file_handle.close()


def encode_enum_column(values: List[Any], compress: bool = True) -> Union[List, Dict]:
    """
    Encode a column with repetitive values using _EnumKey_/_EnumValue_ JData construct.

    If the number of unique values is small relative to total values, encodes as:
    {
        "_EnumKey_": [unique_value1, unique_value2, ...],
        "_EnumValue_": {
            "_ArraySize_": n,
            "_ArrayType_": "uint8"|"uint16"|"uint32",
            "_ArrayZipType_": "zlib",
            "_ArrayZipSize_": n,
            "_ArrayZipData_": base64_encoded_compressed_indices
        }
    }

    Args:
        values: List of column values
        compress: If True, apply zlib compression

    Returns:
        Original list if encoding not beneficial, or dict with _EnumKey_/_EnumValue_
    """
    if not values:
        return values

    # Build enum mapping (preserving order of first occurrence)
    enum_map = OrderedDict()
    indices = []

    for val in values:
        if val not in enum_map:
            enum_map[val] = len(enum_map) + 1  # 1-based indexing like Perl version
        indices.append(enum_map[val])

    num_unique = len(enum_map)
    num_values = len(values)

    # Only encode if beneficial (more values than 2x unique keys)
    if num_values <= 2 * num_unique:
        return values

    # Determine array type based on number of unique values
    if num_unique < 256:
        dtype = np.uint8
        dtype_name = "uint8"
    elif num_unique < 65536:
        dtype = np.uint16
        dtype_name = "uint16"
    else:
        dtype = np.uint32
        dtype_name = "uint32"

    # Convert indices to numpy array and get bytes
    indices_array = np.array(indices, dtype=dtype)
    indices_bytes = indices_array.tobytes()

    if compress:
        compressed = zlib.compress(indices_bytes)
        enum_value = OrderedDict(
            [
                ("_ArraySize_", num_values),
                ("_ArrayType_", dtype_name),
                ("_ArrayZipType_", "zlib"),
                ("_ArrayZipSize_", num_values),
                ("_ArrayZipData_", base64.b64encode(compressed).decode("ascii")),
            ]
        )
    else:
        enum_value = OrderedDict(
            [
                ("_ArraySize_", num_values),
                ("_ArrayType_", dtype_name),
                ("_ArrayData_", base64.b64encode(indices_bytes).decode("ascii")),
            ]
        )

    return OrderedDict(
        [("_EnumKey_", list(enum_map.keys())), ("_EnumValue_", enum_value)]
    )


def decode_enum_column(data: Dict) -> List[Any]:
    """
    Decode a column encoded with _EnumKey_/_EnumValue_ JData construct.

    Args:
        data: Dict with _EnumKey_ and _EnumValue_ keys

    Returns:
        List of decoded values
    """
    if not isinstance(data, dict):
        return data

    if "_EnumKey_" not in data or "_EnumValue_" not in data:
        return data

    enum_keys = data["_EnumKey_"]
    enum_value = data["_EnumValue_"]

    # Get array metadata
    array_size = enum_value.get("_ArraySize_", 0)
    array_type = enum_value.get("_ArrayType_", "uint8")

    # Map type names to numpy dtypes
    dtype_map = {
        "uint8": np.uint8,
        "uint16": np.uint16,
        "uint32": np.uint32,
        "int8": np.int8,
        "int16": np.int16,
        "int32": np.int32,
    }
    dtype = dtype_map.get(array_type, np.uint8)

    # Decode the indices
    if "_ArrayZipData_" in enum_value:
        # Compressed data
        compressed = base64.b64decode(enum_value["_ArrayZipData_"])
        zip_type = enum_value.get("_ArrayZipType_", "zlib")
        if zip_type == "zlib":
            decompressed = zlib.decompress(compressed)
        else:
            raise ValueError(f"Unsupported compression type: {zip_type}")
        indices = np.frombuffer(decompressed, dtype=dtype)
    elif "_ArrayData_" in enum_value:
        # Uncompressed base64 data
        raw_bytes = base64.b64decode(enum_value["_ArrayData_"])
        indices = np.frombuffer(raw_bytes, dtype=dtype)
    else:
        raise ValueError("_EnumValue_ must contain _ArrayZipData_ or _ArrayData_")

    # Map indices back to values (indices are 1-based)
    result = [
        enum_keys[idx - 1] if 0 < idx <= len(enum_keys) else None for idx in indices
    ]

    return result


def is_enum_encoded(data: Any) -> bool:
    """Check if data is encoded with _EnumKey_/_EnumValue_ construct."""
    return isinstance(data, dict) and "_EnumKey_" in data and "_EnumValue_" in data


def tsv2json(
    filepath_or_data: Union[str, List[List]],
    compress: bool = False,
    skip_columns: List[str] = None,
    is_participants: bool = False,
) -> Dict[str, Any]:
    """
    Convert TSV/CSV file or data to JSON with optional enum encoding.

    Args:
        filepath_or_data: File path or list of rows (first row is header)
        compress: If True, use _EnumKey_/_EnumValue_ encoding for repetitive columns
        skip_columns: Column names to skip enum encoding (always keep raw)
        is_participants: If True, skip encoding for age/sex/gender columns

    Returns:
        Dict with column names as keys
    """

    # Load data
    if isinstance(filepath_or_data, str):
        data = load_csv_tsv(filepath_or_data, return_dict=True, convert_numeric=True)
    else:
        # Assume list of rows with header as first row
        rows = filepath_or_data
        if not rows:
            return {}
        headers = rows[0]
        data = {h: [] for h in headers}
        for row in rows[1:]:
            for i, h in enumerate(headers):
                val = row[i] if i < len(row) else ""
                data[h].append(val)

    if not compress:
        return data

    # Apply enum encoding
    skip_columns = skip_columns or []
    result = OrderedDict()

    for col_name, col_values in data.items():
        # Skip encoding for certain columns
        should_skip = col_name in skip_columns
        if is_participants and any(
            x in col_name.lower() for x in ["age", "sex", "gender"]
        ):
            should_skip = True

        if should_skip:
            result[col_name] = col_values
        else:
            result[col_name] = encode_enum_column(col_values, compress=True)

    return result


def json2tsv(
    data: Dict[str, Any],
    filepath: str = None,
    delimiter: str = "\t",
) -> Union[str, None]:
    """
    Convert JSON dict back to TSV/CSV, handling _EnumKey_/_EnumValue_ encoded columns.

    Args:
        data: Dict with column names as keys (may contain enum-encoded columns)
        filepath: Output file path. If None, returns string
        delimiter: Column delimiter (default: tab)

    Returns:
        TSV string if filepath is None, otherwise None (writes to file)
    """
    if not data:
        return "" if filepath is None else None

    # Decode any enum-encoded columns
    decoded_data = OrderedDict()
    for col_name, col_values in data.items():
        if is_enum_encoded(col_values):
            decoded_data[col_name] = decode_enum_column(col_values)
        elif isinstance(col_values, list):
            decoded_data[col_name] = col_values
        else:
            # Single value or other type - wrap in list
            decoded_data[col_name] = [col_values]

    # Get column names and row count
    col_names = list(decoded_data.keys())
    if not col_names:
        return "" if filepath is None else None

    first_col = decoded_data[col_names[0]]
    num_rows = len(first_col) if isinstance(first_col, list) else 1

    # Build output lines
    lines = []

    # Header
    lines.append(delimiter.join(str(c) for c in col_names))

    # Data rows
    for i in range(num_rows):
        row = []
        for col_name in col_names:
            col_values = decoded_data[col_name]
            if isinstance(col_values, list) and i < len(col_values):
                val = col_values[i]
            elif isinstance(col_values, np.ndarray) and i < len(col_values):
                val = col_values[i]
            else:
                val = ""

            # Handle None and numpy types
            if val is None:
                val = ""
            elif isinstance(val, (np.integer, np.floating)):
                val = val.item()
            elif isinstance(val, float) and np.isnan(val):
                val = "n/a"

            row.append(str(val))
        lines.append(delimiter.join(row))

    output = "\n".join(lines) + "\n"

    if filepath:
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(output)
        return None

    return output


# =============================================================================
# Update to save_csv_tsv - add compress parameter
# =============================================================================


def save_csv_tsv_with_enum(
    data: Union[Dict[str, List], List[List]],
    filename: str,
    delimiter: str = None,
    compress_enum: bool = False,
    skip_columns: List[str] = None,
    is_participants: bool = False,
    **kwargs,
):
    """
    Write data to CSV/TSV files with optional enum encoding for JSON output.

    This is an enhanced version of save_csv_tsv that supports:
    - Regular CSV/TSV output
    - JSON output with _EnumKey_/_EnumValue_ encoding for repetitive columns

    Args:
        data: Dictionary with column data or list of rows
        filename: Output file path
        delimiter: Column delimiter. If None, auto-detect from filename
        compress_enum: If True and filename ends with .json, use enum encoding
        skip_columns: Column names to skip enum encoding
        is_participants: If True, skip encoding for age/sex/gender columns
        **kwargs: Additional arguments for csv.writer
    """
    # Check if JSON output requested
    if filename.lower().endswith(".json") and compress_enum:
        import json

        if isinstance(data, list):
            # Convert list of rows to dict
            headers = data[0] if data else []
            dict_data = {h: [] for h in headers}
            for row in data[1:]:
                for i, h in enumerate(headers):
                    dict_data[h].append(row[i] if i < len(row) else "")
            data = dict_data

        # Apply enum encoding
        encoded = OrderedDict()
        skip_columns = skip_columns or []

        for col_name, col_values in data.items():
            should_skip = col_name in skip_columns
            if is_participants and any(
                x in col_name.lower() for x in ["age", "sex", "gender"]
            ):
                should_skip = True

            if should_skip or not compress_enum:
                encoded[col_name] = col_values
            else:
                encoded[col_name] = encode_enum_column(col_values, compress=True)

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(encoded, f, separators=(",", ":"))
    else:
        # Regular CSV/TSV output - use existing save_csv_tsv
        save_csv_tsv(data, filename, delimiter=delimiter, **kwargs)
